{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Checkpoint:</h1>\n",
    "\n",
    "**Looking to see completetion and effort in completing the checkpoint. It's okay if it's not correct**\n",
    "\n",
    "Based off this dataset with school financial, enrollment, and achievement data, we are interested in what information is a useful indicator of student performance at the state level.\n",
    "\n",
    "This question is a bit too big for a checkpoint, however. Instead, we want you to look at smaller questions related to our overall goal. Here's the overview:\n",
    "\n",
    "1. Choose a specific test to focus on\n",
    ">Math/Reading for 4/8 grade\n",
    "* Pick or create features to use\n",
    ">Will all the features be useful in predicting test score? Are some more important than others? Should you standardize, bin, or scale the data?\n",
    "* Explore the data as it relates to that test\n",
    ">Create 2 well-labeled visualizations (graphs), each with a caption describing the graph and what it tells us about the data\n",
    "* Create training and testing data\n",
    ">Do you want to train on all the data? Only data from the last 10 years? Only Michigan data?\n",
    "* Train a ML model to predict outcome \n",
    ">Pick if you want to do a regression or classification task. For both cases, defined _exactly_ what you want to predict, and pick any model in sklearn to use (see sklearn <a href=\"https://scikit-learn.org/stable/modules/linear_model.html\">regressors</a> and <a href=\"https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\">classifiers</a>).\n",
    "* Summarize your findings\n",
    ">Write a 1 paragraph summary of what you did and make a recommendation about if and how student performance can be predicted\n",
    "\n",
    "** Include comments throughout your code! Every cleanup and preprocessing task should be documented.\n",
    "\n",
    "\n",
    "Of course, if you're finding this assignment interesting (and we really hope you do!), you are welcome to do more than the requirements! For example, you may want to see if expenditure affects 4th graders more than 8th graders. Maybe you want to look into the extended version of this dataset and see how factors like sex and race are involved. You can include all your work in this notebook when you turn it in -- just always make sure you explain what you did and interpret your results. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Cleanup </h2>\n",
    "\n",
    "Import numpy, pandas, matplotlib, and seaborn\n",
    "\n",
    "(Feel free to import other libraries!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import matplotlib.pyplot as plt;\n",
    "import seaborn as sb;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the \"states_edu.csv\" dataset and take a look at the head of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/states_edu.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always familiarize yourself with what each column in the dataframe represents. \\ Read about the states_edu dataset here: https://www.kaggle.com/noriuk/us-education-datasets-unification-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this space to rename columns, deal with missing data, etc. _(optional)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1715, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's rename our columns to make them more intuitive\n",
    "df.rename({\n",
    "    'GRADES_PK_G':'ENROLL_PREK',\n",
    "    'GRADES_KG_G':'ENROLL_KINDER',\n",
    "    'GRADES_4_G':'ENROLL_4',\n",
    "    'GRADES_8_G':'ENROLL_8',\n",
    "    'GRADES_12_G':'ENROLL_12',\n",
    "    'GRADES_1_8_G':'ENROLL_PRIMARY',\n",
    "    'GRADES_9_12_G':'ENROLL_HS',\n",
    "    'GRADES_ALL_G':'ENROLL_ALL',\n",
    "    'ENROLL':'ENROLL_ALL_EST'\n",
    "    },\n",
    "    axis=1,inplace=True)\n",
    "#inplace return copy of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exploratory Data Analysis (EDA) </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chosen Outcome Variable for Test: **<Math/Reading for 4/8 grade>**   (Ex. Math for 8th grade)\n",
    "\n",
    "**(hit `Enter` to edit)**\n",
    "\n",
    "Outcome Score in the questions refers to the outcome variable you chose here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many different years of data are in our dataset? Use a pandas function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002,\n",
       "       2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013,\n",
       "       2014, 2015, 2016, 1986, 1987, 1988, 1989, 1990, 1991, 2017, 2019])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"YEAR\"].unique()\n",
    "#from 1992 to 2019\n",
    "#totally 33 different years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare Michigan to Ohio. Which state has the higher average outcome score across all years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"STATE\"][\"Michigan\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the average for your outcome score across all states in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame._add_numeric_operations.<locals>.mean of              PRIMARY_KEY          STATE  YEAR  ENROLL_ALL_EST  TOTAL_REVENUE  \\\n",
       "0           1992_ALABAMA        ALABAMA  1992             NaN      2678885.0   \n",
       "1            1992_ALASKA         ALASKA  1992             NaN      1049591.0   \n",
       "2           1992_ARIZONA        ARIZONA  1992             NaN      3258079.0   \n",
       "3          1992_ARKANSAS       ARKANSAS  1992             NaN      1711959.0   \n",
       "4        1992_CALIFORNIA     CALIFORNIA  1992             NaN     26260025.0   \n",
       "...                  ...            ...   ...             ...            ...   \n",
       "1710       2019_VIRGINIA       VIRGINIA  2019             NaN            NaN   \n",
       "1711     2019_WASHINGTON     WASHINGTON  2019             NaN            NaN   \n",
       "1712  2019_WEST_VIRGINIA  WEST_VIRGINIA  2019             NaN            NaN   \n",
       "1713      2019_WISCONSIN      WISCONSIN  2019             NaN            NaN   \n",
       "1714        2019_WYOMING        WYOMING  2019             NaN            NaN   \n",
       "\n",
       "      FEDERAL_REVENUE  STATE_REVENUE  LOCAL_REVENUE  TOTAL_EXPENDITURE  \\\n",
       "0            304177.0      1659028.0       715680.0          2653798.0   \n",
       "1            106780.0       720711.0       222100.0           972488.0   \n",
       "2            297888.0      1369815.0      1590376.0          3401580.0   \n",
       "3            178571.0       958785.0       574603.0          1743022.0   \n",
       "4           2072470.0     16546514.0      7641041.0         27138832.0   \n",
       "...               ...            ...            ...                ...   \n",
       "1710              NaN            NaN            NaN                NaN   \n",
       "1711              NaN            NaN            NaN                NaN   \n",
       "1712              NaN            NaN            NaN                NaN   \n",
       "1713              NaN            NaN            NaN                NaN   \n",
       "1714              NaN            NaN            NaN                NaN   \n",
       "\n",
       "      INSTRUCTION_EXPENDITURE  ...  ENROLL_4  ENROLL_8  ENROLL_12  \\\n",
       "0                   1481703.0  ...   57948.0   58025.0    41167.0   \n",
       "1                    498362.0  ...    9748.0    8789.0     6714.0   \n",
       "2                   1435908.0  ...   55433.0   49081.0    37410.0   \n",
       "3                    964323.0  ...   34632.0   36011.0    27651.0   \n",
       "4                  14358922.0  ...  418418.0  363296.0   270675.0   \n",
       "...                       ...  ...       ...       ...        ...   \n",
       "1710                      NaN  ...       NaN       NaN        NaN   \n",
       "1711                      NaN  ...       NaN       NaN        NaN   \n",
       "1712                      NaN  ...       NaN       NaN        NaN   \n",
       "1713                      NaN  ...       NaN       NaN        NaN   \n",
       "1714                      NaN  ...       NaN       NaN        NaN   \n",
       "\n",
       "      ENROLL_PRIMARY  ENROLL_HS  ENROLL_ALL  AVG_MATH_4_SCORE  \\\n",
       "0                NaN        NaN    731634.0             208.0   \n",
       "1                NaN        NaN    122487.0               NaN   \n",
       "2                NaN        NaN    673477.0             215.0   \n",
       "3                NaN        NaN    441490.0             210.0   \n",
       "4                NaN        NaN   5254844.0             208.0   \n",
       "...              ...        ...         ...               ...   \n",
       "1710             NaN        NaN         NaN             247.0   \n",
       "1711             NaN        NaN         NaN             240.0   \n",
       "1712             NaN        NaN         NaN             231.0   \n",
       "1713             NaN        NaN         NaN             242.0   \n",
       "1714             NaN        NaN         NaN             246.0   \n",
       "\n",
       "      AVG_MATH_8_SCORE  AVG_READING_4_SCORE  AVG_READING_8_SCORE  \n",
       "0                252.0                207.0                  NaN  \n",
       "1                  NaN                  NaN                  NaN  \n",
       "2                265.0                209.0                  NaN  \n",
       "3                256.0                211.0                  NaN  \n",
       "4                261.0                202.0                  NaN  \n",
       "...                ...                  ...                  ...  \n",
       "1710             287.0                224.0                262.0  \n",
       "1711             286.0                220.0                266.0  \n",
       "1712             272.0                213.0                256.0  \n",
       "1713             289.0                220.0                267.0  \n",
       "1714             286.0                227.0                265.0  \n",
       "\n",
       "[1715 rows x 25 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note - this test is scored out of 500 according to the NAEP website\n",
    "df.AVG_READING_8_SCORE.hist()\n",
    "plt.xlabel('score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of 8th grade reading scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the maximum outcome score for every state. Hint: there's a function that allows you to do this easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Feature Selection </h2>\n",
    "\n",
    "After exploring the data, you now have to choose features that you would use to predict the performance of the students on a chosen test (chosen outcome variable). By the way, you can also create your own features. For example, perhaps you figured that maybe a state's expenditure per student may affect their overall academic performance so you create a expenditure_per_student feature.\n",
    "\n",
    "Use this space to modify or create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final feature list: **<LIST FEATURES HERE\\>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection justification: **<BRIEFLY DESCRIBE WHY YOU PICKED THESE FEATURES\\>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Visualization</h2>\n",
    "\n",
    "Use any graph you wish to see the relationship of your chosen outcome variable with any features you chose\n",
    "\n",
    "**Visualization 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<CAPTION FOR VIZ 1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<CAPTION FOR VIZ 2>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Creation </h2>\n",
    "\n",
    "_Use this space to create train/test data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ??\n",
    "y = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=??, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prediction </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Models Resource: https://medium.com/@vijaya.beeravalli/comparison-of-machine-learning-classification-models-for-credit-card-default-data-c3cf805c9a5a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chosen ML task: **<REGRESSION/CLASSIFICATION>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import your sklearn class here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your model here\n",
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR CLASSIFICATION ONLY:\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(model, X_test, y_test,\n",
    "                         cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR REGRESSION ONLY: (pick a single column to visualize results)\n",
    "\n",
    "# Results from this graph _should not_ be used as a part of your results -- it is just here to help with intuition. \n",
    "# Instead, look at the error values and individual intercepts.\n",
    "\n",
    "\n",
    "col_name = ??\n",
    "col_index = X_train.columns.get_loc(col_name)\n",
    "\n",
    "f = plt.figure(figsize=(12,6))\n",
    "plt.scatter(X_train[col_name], y_train, color = \"red\")\n",
    "plt.scatter(X_train[col_name], model.predict(X_train), color = \"green\")\n",
    "plt.scatter(X_test[col_name], model.predict(X_test), color = \"blue\")\n",
    "\n",
    "new_x = np.linspace(X_train[col_name].min(),X_train[col_name].max(),200)\n",
    "intercept = model.predict([X_train.sort_values(col_name).iloc[0]]) - X_train[col_name].min()*model.coef_[col_index]\n",
    "plt.plot(new_x, intercept+new_x*model.coef_[col_index])\n",
    "\n",
    "plt.legend(['controlled model','true training','predicted training','predicted testing'])\n",
    "plt.xlabel(col_name)\n",
    "plt.ylabel(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Summary </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<WRITE A PARAGRAPH SUMMARIZING YOUR WORK AND FINDINGS\\>**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
